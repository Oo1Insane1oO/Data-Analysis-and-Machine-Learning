{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on Machine Learning\n",
    "## Overview\n",
    "The aim of this project is to use data from Monte Carlo simulations of a familiar system from Statistical Mechanics, namely the Ising Model. We will use a simple model without any external magnetic field. The energy expectation value is expressed as\n",
    "    $$E=-J\\sum\\limits^N_{\\{kl\\}}s_ks_l$$\n",
    "The $s_k$ and $s_l$ indicate a spin. The spins are represented in a spin-lattice with $s_k=\\pm 1$ and $N$ being the total number of spins. $J$ is a coupling constant representing the strength of the interaction between neighbouring pairs of spins. The $<kl>$ notation indicate sum over the nearest neighbours.\n",
    "\n",
    "The data used is and the methods explored follow closesly article >> ref <<. The methods explored here is logistic regression, random forest algortihm and deep neural networks.\n",
    "\n",
    "The interresting physical properties to be extracted is states above, below and around a critical temperature $T_c$. When the system is in a temperature lower than this the system is in a so-called ferromagnetic phase. When close to the critical point, the magnetization becomes smaller, while the net magnetization is zero when the temperature is above $T_c$.\n",
    "\n",
    "## Theory\n",
    "We will first present the theory for the methods mentioned.\n",
    "\n",
    "### Linear Regression\n",
    "Linear regression model is a model for fitting data-points to a linear functional form.\n",
    "\n",
    "Given a data set \n",
    "    $$\\{y_i, \\boldsymbol{x}_i\\}_{i=1}^n,\\; i=1,\\dots,n$$ \n",
    "of $n$ points with $\\boldsymbol{X}$ being the $n\\times m$ matrix representing the regressors. Assuming the relationship between the regressors and $y_i$ is linear, the model is\n",
    "    $$\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$$\n",
    "with \n",
    "    $$\n",
    "    \\boldsymbol{y} =\n",
    "        \\begin{pmatrix}\n",
    "            y_1 \\\\\n",
    "            \\vdots \\\\\n",
    "            y_n\n",
    "        \\end{pmatrix},\n",
    "    $$\n",
    "    $$\n",
    "    \\boldsymbol{X} =\n",
    "        \\begin{pmatrix}\n",
    "            1 & f_1\\left(x_{11}\\right) & \\dots & f_1\\left(x_{1m}\\right) \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            1 & f_n\\left(x_{n1}\\right) & \\dots & f_n\\left(x_{nm}\\right)\n",
    "        \\end{pmatrix}\n",
    "    $$\n",
    "and\n",
    "    $$\n",
    "    \\boldsymbol{\\beta} = \n",
    "        \\begin{pmatrix}\n",
    "            \\beta_0 \\\\\n",
    "            \\vdots \\\\\n",
    "            \\beta_m\n",
    "        \\end{pmatrix}.\n",
    "    $$\n",
    "The vector $\\boldsymbol{\\varepsilon}$ is an estimate for the noise in the system(i.e variance in the Monte Carlo simulation) and $f_i$ is a pre-defined function. This can for instance be a polynomial function\n",
    "    $$f_i(x) = x^i,$$\n",
    "or a polynomial in sine\n",
    "    $$f_i(x) = \\sin(ix),$$\n",
    "or any other suitable choice.\n",
    "\n",
    "The method of linear regression is simply to minimize with respect to parameters $\\boldsymbol{\\beta}$.\n",
    "\n",
    "#### Ridge and Lasso Regression\n",
    "While the linear regression model is rigorous and simple, it does have a tendency to overfit. In order to somewhat avoid this problem so-called regularization technicues have been developed. Two of these are Ridge and Lasso Regression.\n",
    "\n",
    "##### Ridge Regression\n",
    "With Ridge regression one performs an L2 regularization by adding an additional term equal to the square of the magnitude of the coefficients. This effectively ends up with performing the original linear regression, but with an added term. The equation is as follows\n",
    "    $$\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\alpha\\sum_{i=1}^m\\beta^2_i.$$\n",
    "The factor $\\alpha$ is just a scaling.\n",
    "\n",
    "##### Lasso Regression\n",
    "The Lasso regression scheme performs an L1 regularization by adding only the absolute value of the magnitude of coefficients. The equation is\n",
    "    $$\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\alpha\\sum_{i=1}^m{\\big|}\\beta_i{\\big|},$$\n",
    "with $\\alpha$ defined as before.\n",
    "\n",
    "#### Transform the Ising Model to a Linear Regression Problem\n",
    "In order to use linear regression with the Ising model we assume the model (without any prior knowledge) the all-to-all Ising model\n",
    "    $$E^{(i)} = -\\sum\\limits_{kl}^NJ_{kl}s^{(i)}_ks^{(i)}_l,$$\n",
    "with the $J_{kl}$ being the coupling strengths we wish to learn. The index $i$ represents a sample point. This equation can be rewritten as the matrix equation\n",
    "    $$E^{(i)} = -\\boldsymbol{X}^{(i)} \\cdot \\boldsymbol{J},$$\n",
    "with $\\boldsymbol{X}^{(i)}$ representing the two-body interactions \n",
    "    $$\\left\\{s^{(i)}_k,s^{(i)}_l\\right\\}_{k,l=1}^N.$$\n",
    "\n",
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
